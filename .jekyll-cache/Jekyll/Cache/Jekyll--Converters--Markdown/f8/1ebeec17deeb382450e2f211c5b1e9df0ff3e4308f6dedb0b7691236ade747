I"ı<h2 id="the-role-of-world-models-and-abstraction-for-neural-network-agents">The Role of World Models and Abstraction for Neural Network Agents</h2>

<h3 id="presenter--Ë¨õÊºîËÄÖ">Presenter | Ë¨õÊºîËÄÖ</h3>

<h4 id="david-ha">David Ha</h4>

<p>Research Scientist at Google</p>

<h3 id="biography--Áï•Ê≠¥">Biography | Áï•Ê≠¥</h3>

<p>David is a Research Scientist at Google Brain in Tokyo, Japan. His research interests include Neural Networks, Creative AI, and Evolutionary Computing. Prior to joining Google, He worked at Goldman Sachs as a Managing Director, where he ran the fixed-income trading business in Japan. He obtained undergraduate and graduate degrees in Engineering Science and Applied Math from the University of Toronto.</p>

<p>https://otoro.net/ml/</p>

<h3 id="abstract--Ê¶ÇË¶Å">Abstract | Ê¶ÇË¶Å</h3>

<p>In the past decade we saw great improvements in neural networks applied to machine learning. When combined with reinforcement learning, neural networks are able learn tasks that were previously difficult with hand-engineered approaches. However, neural network policies are often brittle in nature and also do not generalize well beyond a narrow domain environment, like what we expect of humans. In this talk I will examine a line of work inspired by cognitive neuroscience, enabling artificial agents to possess a world model of its environment. I will examine the implications and explore future possibilities of such learnable world models and how they may affect agents able to exploit them.</p>
:ET