<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ja"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="ja" /><updated>2021-11-09T22:51:19+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">NeurIPS Meetup Japan</title><subtitle>Neural Information Processing Systems Official Meetup in Japan.</subtitle><author><name>NeurIPS Meetup Japan Organizers</name></author><entry><title type="html">Invited Talk “Reinforcement Learning Straight Up and Upside Down”</title><link href="http://localhost:4000/talk/2020/12/12/talk5.html" rel="alternate" type="text/html" title="Invited Talk “Reinforcement Learning Straight Up and Upside Down”" /><published>2020-12-12T15:30:00+09:00</published><updated>2020-12-12T15:30:00+09:00</updated><id>http://localhost:4000/talk/2020/12/12/talk5</id><content type="html" xml:base="http://localhost:4000/talk/2020/12/12/talk5.html">&lt;h2 id=&quot;reinforcement-learning-straight-up-and-upside-down&quot;&gt;Reinforcement Learning: Straight Up and Upside Down&lt;/h2&gt;

&lt;h3 id=&quot;presenter--講演者&quot;&gt;Presenter | 講演者&lt;/h3&gt;

&lt;h4 id=&quot;kai-arulkumaran&quot;&gt;Kai Arulkumaran&lt;/h4&gt;

&lt;p&gt;Researcher at Araya&lt;/p&gt;

&lt;h3 id=&quot;biography--略歴&quot;&gt;Biography | 略歴&lt;/h3&gt;

&lt;p&gt;Kai is a researcher at Araya, where he works on deep learning, reinforcement learning, and neuroscience. He received his BA in Computer Science at the University of Cambridge in 2012 and his PhD in Bioengineering from Imperial College London in 2020. In the past he has worked at DeepMind, Microsoft Research, Facebook AI Research, Twitter and NNAISENSE, and was also a mentor for the OpenAI Scholars program.&lt;/p&gt;

&lt;h3 id=&quot;abstract--概要&quot;&gt;Abstract | 概要&lt;/h3&gt;

&lt;p&gt;Reinforcement learning is the study of sequential decision making, with the “simple” objective of maximising long-term reward. In this talk I will introduce the two main approaches to solving reinforcement learning, evaluating their pros and cons. To finish, I will introduce upside down reinforcement learning, which may provide a simplified approach to tackling many traditional reinforcement learning problems.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="talk" /><category term="talk" /><category term="day3" /><summary type="html">Reinforcement Learning: Straight Up and Upside Down</summary></entry><entry><title type="html">Coffee Break</title><link href="http://localhost:4000/social/2020/12/12/coffee.html" rel="alternate" type="text/html" title="Coffee Break" /><published>2020-12-12T15:00:00+09:00</published><updated>2020-12-12T15:00:00+09:00</updated><id>http://localhost:4000/social/2020/12/12/coffee</id><content type="html" xml:base="http://localhost:4000/social/2020/12/12/coffee.html">&lt;p&gt;To be determined.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="social" /><category term="social" /><category term="break" /><category term="day3" /><summary type="html">To be determined.</summary></entry><entry><title type="html">Meet the Authors 6 - Isao Ishikawa</title><link href="http://localhost:4000/discussion/2020/12/12/study6.html" rel="alternate" type="text/html" title="Meet the Authors 6 - Isao Ishikawa" /><published>2020-12-12T14:30:00+09:00</published><updated>2020-12-12T14:30:00+09:00</updated><id>http://localhost:4000/discussion/2020/12/12/study6</id><content type="html" xml:base="http://localhost:4000/discussion/2020/12/12/study6.html">&lt;h4 id=&quot;takeshi-teshima-isao-ishikawa-koichi-tojo-kenta-oono-masahiro-ikeda-masashi-sugiyama-coupling-based-invertible-neural-networks-are-universal-diffeomorphism-approximators&quot;&gt;Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, Masashi Sugiyama &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/2290a7385ed77cc5592dc2153229f082-Abstract.html&quot;&gt;Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;Invertible neural networks based on coupling flows (CF-INNs) have various machine learning applications such as image synthesis and representation learning. However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their representation power: are CF-INNs universal approximators for invertible functions? Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain affine coupling and invertible linear functions as special cases. As its corollary, we can affirmatively resolve a previously unsolved problem: whether normalizing flow models based on affine coupling can be universal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="discussion" /><category term="discussion" /><category term="day3" /><summary type="html">Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, Masashi Sugiyama Coupling-based Invertible Neural Networks Are Universal Diffeomorphism Approximators</summary></entry><entry><title type="html">Meet the Authors 5 - Kenta Oono</title><link href="http://localhost:4000/discussion/2020/12/12/study5.html" rel="alternate" type="text/html" title="Meet the Authors 5 - Kenta Oono" /><published>2020-12-12T14:00:00+09:00</published><updated>2020-12-12T14:00:00+09:00</updated><id>http://localhost:4000/discussion/2020/12/12/study5</id><content type="html" xml:base="http://localhost:4000/discussion/2020/12/12/study5.html">&lt;h4 id=&quot;kenta-oono-taiji-suzuki-optimization-and-generalization-analysis-of-transduction-through-gradient-boosting-and-application-to-multi-scale-graph-neural-networks&quot;&gt;Kenta Oono, Taiji Suzuki &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/dab49080d80c724aad5ebf158d63df41-Abstract.html&quot;&gt;Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;It is known that the current graph neural networks (GNNs) are difficult to make themselves deep due to the problem known as over-smoothing. Multi-scale GNNs are a promising approach for mitigating the over-smoothing problem. However, there is little explanation of why it works empirically from the viewpoint of learning theory. In this study, we derive the optimization and generalization guarantees of transductive learning algorithms that include multi-scale GNNs. Using the boosting theory, we prove the convergence of the training error under weak learning-type conditions. By combining it with generalization gap bounds in terms of transductive Rademacher complexity, we show that a test error bound of a specific type of multi-scale GNNs that decreases corresponding to the number of node aggregations under some conditions. Our results offer theoretical explanations for the effectiveness of the multi-scale structure against the over-smoothing problem. We apply boosting algorithms to the training of multi-scale GNNs for real-world node prediction tasks. We confirm that its performance is comparable to existing GNNs, and the practical behaviors are consistent with theoretical observations. Code is available at https://github.com/delta2323/GB-GNN.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="discussion" /><category term="discussion" /><category term="day3" /><summary type="html">Kenta Oono, Taiji Suzuki Optimization and Generalization Analysis of Transduction through Gradient Boosting and Application to Multi-scale Graph Neural Networks</summary></entry><entry><title type="html">Meet the Authors 4 - Motoya Ohnishi</title><link href="http://localhost:4000/discussion/2020/12/12/study4.html" rel="alternate" type="text/html" title="Meet the Authors 4 - Motoya Ohnishi" /><published>2020-12-12T13:30:00+09:00</published><updated>2020-12-12T13:30:00+09:00</updated><id>http://localhost:4000/discussion/2020/12/12/study4</id><content type="html" xml:base="http://localhost:4000/discussion/2020/12/12/study4.html">&lt;h4 id=&quot;sham-kakade-akshay-krishnamurthy-kendall-lowrey-motoya-ohnishi-wen-sun-information-theoretic-regret-bounds-for-online-nonlinear-control&quot;&gt;Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, Wen Sun &lt;a href=&quot;https://papers.nips.cc/paper/2020/hash/aee5620fa0432e528275b8668581d9a8-Abstract.html&quot;&gt;Information Theoretic Regret Bounds for Online Nonlinear Control&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;This work studies the problem of sequential control in an unknown, nonlinear dynamical system, where we model the underlying system dynamics as an unknown function in a known Reproducing Kernel Hilbert Space. This framework yields a general setting that permits discrete and continuous control inputs as well as non-smooth, non-differentiable dynamics. Our main result, the Lower Confidence-based Continuous Control (LC3) algorithm, enjoys a near-optimal O(√T) regret bound against the optimal controller in episodic settings, where T is the number of episodes. The bound has no explicit dependence on dimension of the system dynamics, which could be infinite, but instead only depends on information theoretic quantities. We empirically show its application to a number of nonlinear control tasks and demonstrate the benefit of exploration for learning model dynamics.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="discussion" /><category term="discussion" /><category term="day3" /><summary type="html">Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, Wen Sun Information Theoretic Regret Bounds for Online Nonlinear Control</summary></entry><entry><title type="html">Lunch Break</title><link href="http://localhost:4000/social/2020/12/12/break.html" rel="alternate" type="text/html" title="Lunch Break" /><published>2020-12-12T11:30:00+09:00</published><updated>2020-12-12T11:30:00+09:00</updated><id>http://localhost:4000/social/2020/12/12/break</id><content type="html" xml:base="http://localhost:4000/social/2020/12/12/break.html">&lt;p&gt;To be determined.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="social" /><category term="social" /><category term="break" /><category term="day3" /><summary type="html">To be determined.</summary></entry><entry><title type="html">Keynote 2 - TBD</title><link href="http://localhost:4000/keynote/2020/12/12/keynote2.html" rel="alternate" type="text/html" title="Keynote 2 - TBD" /><published>2020-12-12T10:00:00+09:00</published><updated>2020-12-12T10:00:00+09:00</updated><id>http://localhost:4000/keynote/2020/12/12/keynote2</id><content type="html" xml:base="http://localhost:4000/keynote/2020/12/12/keynote2.html">&lt;p&gt;To be determined.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="keynote" /><category term="talk" /><category term="keynote" /><category term="day3" /><summary type="html">To be determined.</summary></entry><entry><title type="html">Invited Talk “The Role of World Models and Abstraction for Neural Network Agents”</title><link href="http://localhost:4000/talk/2020/12/09/talk4.html" rel="alternate" type="text/html" title="Invited Talk “The Role of World Models and Abstraction for Neural Network Agents”" /><published>2020-12-09T16:30:00+09:00</published><updated>2020-12-09T16:30:00+09:00</updated><id>http://localhost:4000/talk/2020/12/09/talk4</id><content type="html" xml:base="http://localhost:4000/talk/2020/12/09/talk4.html">&lt;h2 id=&quot;the-role-of-world-models-and-abstraction-for-neural-network-agents&quot;&gt;The Role of World Models and Abstraction for Neural Network Agents&lt;/h2&gt;

&lt;h3 id=&quot;presenter--講演者&quot;&gt;Presenter | 講演者&lt;/h3&gt;

&lt;h4 id=&quot;david-ha&quot;&gt;David Ha&lt;/h4&gt;

&lt;p&gt;Research Scientist at Google&lt;/p&gt;

&lt;h3 id=&quot;biography--略歴&quot;&gt;Biography | 略歴&lt;/h3&gt;

&lt;p&gt;David is a Research Scientist at Google Brain in Tokyo, Japan. His research interests include Neural Networks, Creative AI, and Evolutionary Computing. Prior to joining Google, He worked at Goldman Sachs as a Managing Director, where he ran the fixed-income trading business in Japan. He obtained undergraduate and graduate degrees in Engineering Science and Applied Math from the University of Toronto.&lt;/p&gt;

&lt;p&gt;https://otoro.net/ml/&lt;/p&gt;

&lt;h3 id=&quot;abstract--概要&quot;&gt;Abstract | 概要&lt;/h3&gt;

&lt;p&gt;In the past decade we saw great improvements in neural networks applied to machine learning. When combined with reinforcement learning, neural networks are able learn tasks that were previously difficult with hand-engineered approaches. However, neural network policies are often brittle in nature and also do not generalize well beyond a narrow domain environment, like what we expect of humans. In this talk I will examine a line of work inspired by cognitive neuroscience, enabling artificial agents to possess a world model of its environment. I will examine the implications and explore future possibilities of such learnable world models and how they may affect agents able to exploit them.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="talk" /><category term="talk" /><category term="day2" /><summary type="html">The Role of World Models and Abstraction for Neural Network Agents</summary></entry><entry><title type="html">Coffee Break</title><link href="http://localhost:4000/social/2020/12/09/coffee.html" rel="alternate" type="text/html" title="Coffee Break" /><published>2020-12-09T14:30:00+09:00</published><updated>2020-12-09T14:30:00+09:00</updated><id>http://localhost:4000/social/2020/12/09/coffee</id><content type="html" xml:base="http://localhost:4000/social/2020/12/09/coffee.html">&lt;p&gt;To be determined.&lt;/p&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="social" /><category term="social" /><category term="break" /><category term="day2" /><summary type="html">To be determined.</summary></entry><entry><title type="html">Women in Machine Learning</title><link href="http://localhost:4000/talk/2020/12/09/wiml.html" rel="alternate" type="text/html" title="Women in Machine Learning" /><published>2020-12-09T14:30:00+09:00</published><updated>2020-12-09T14:30:00+09:00</updated><id>http://localhost:4000/talk/2020/12/09/wiml</id><content type="html" xml:base="http://localhost:4000/talk/2020/12/09/wiml.html">&lt;p&gt;Women in ML (WiML) aims to enhance the experience of women in machine learning by connecting wide range of participants from those who are new to ML to top ML researchers and engineers.&lt;/p&gt;

&lt;p&gt;Women in ML (WiML)は機械学習に関わる女性をつなぎ，女性が機械学習コミュニティに参加しやすい環境を整備することを目指しています．&lt;/p&gt;

&lt;p&gt;Registration page for WiML is &lt;a href=&quot;https://forms.gle/4min7ZWcbwQCPNNMA&quot;&gt;here&lt;/a&gt;. The capacity for WiML is set to 25 participants on a first come, first serve basis.&lt;/p&gt;

&lt;p&gt;WiMLへの参加は&lt;a href=&quot;https://forms.gle/4min7ZWcbwQCPNNMA&quot;&gt;こちら&lt;/a&gt;から登録をお願いします．WiMLは先着25名までの参加とさせて頂きます.&lt;/p&gt;

&lt;h3 id=&quot;contents--内容&quot;&gt;Contents | 内容&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;2:30 pm – Welcome words&lt;/li&gt;
  &lt;li&gt;2:35 pm – Blitz 自己紹介 – getting to know each other. Self-introduction by participants.&lt;/li&gt;
  &lt;li&gt;3:15 pm – Break out rooms&lt;/li&gt;
  &lt;li&gt;3:55 pm – Closing remarks&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;organizers&quot;&gt;Organizers&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Suzana Ilic (MLTokyo)&lt;/li&gt;
  &lt;li&gt;Kuroki Yuko (UTokyo, RIKEN AIP) | 黒木祐子 (東京大学, 理研AIP)&lt;/li&gt;
  &lt;li&gt;Nan Lu (UTokyo, RIKEN AIP) | 鲁楠 (東京大学，理研AIP)&lt;/li&gt;
  &lt;li&gt;Hashimoto Yuka (NTT, Keio Univ) | 橋本悠香 (NTT, 慶應義塾大学)&lt;/li&gt;
&lt;/ul&gt;</content><author><name>NeurIPS Meetup Japan Organizers</name></author><category term="talk" /><category term="discussion" /><category term="social" /><category term="day2" /><summary type="html">Women in ML (WiML) aims to enhance the experience of women in machine learning by connecting wide range of participants from those who are new to ML to top ML researchers and engineers.</summary></entry></feed>